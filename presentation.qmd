---
title: "Dynamic Web Scraping with Selenider"
author: 
    - "Paul Elvis Otto"
    - "Polina Ianina"
    - "Aditi Joshi"
format:
  revealjs: 
    theme: [dark, styles.scss]
    title-slide-attributes: 
      data-background-color: "#4e4e4e"
      data-background-image: none

eval: false
---

# Introduction 

## Static and Dynamic?


:::{.center}
:::{.fragment .fade-in-then-semi-out}

What is the difference?
:::
:::

---



# {.center}

:::::{.fragment .fade-in}

:::: {.columns}

::: {.column width="50%" .incremental}
Classical Webscraping

- HTML is static and fully loaded on the server-side
- Calssical usecases: Blogs, Newspaper 
- No interaction with the elements of the site to load content
:::

::: {.column width="50%" .incremental}
Dynamic Webscraping

- Content is rendered via JavaScript on the client side
- Infinite scroling sites, Single page applications
- Interactions like, click scroll and so on

:::

::::

:::::


# So whats the problem here? 

## So whats the problem here?

::: {.fragment .fade-in}
::: {.fragment .highlight-red}
::: {.fragment .semi-fade-out}
When content is not rendered, we cant just access it
:::
:::
:::

::: {.fragment .fade-in}
::: {.fragment .highlight-green}
therefore we need to simulate the interaction, like a person with a browser
:::
:::

# How can we do that?

# With Selenider



## Why Selenider?

*   **Selenider** is an R package for browser automation and web scraping
*   It provides a simple and powerful way to interact with websites like a human user
*   We can select elements, extract data, and even simulate user actions like clicking and scroling

# Setting Up Selenider

## Installation{auto-animate="true"}
Selenider often throws problems when installing, to tackle these we will do the following

## Installation{auto-animate="true"}

Selenider often throws problems when installing, to tackle these we will do the following

```{.r code-line-numbers="1|2|3|"}
install.packages("renv")
library(renv)
renv::init()
renv::activate()
```

## Installation{auto-animate="true"}

Selenider often throws problems when installing, to tackle these we will do the following

```{.r code-line-numbers="5|6|7|"}
install.packages("renv")
library(renv)
renv::init()
renv::activate()
# Selenider also needs a webdirver to run
install.packages("selenider")
install.packages("chromote")

```

## Starting a Session {auto-animate="true"}


*   Load the library and start a session:

```{.r}
library(selenider)
```

## Starting a Session {auto-animate="true"}

*   Load the library and start a session:

```{.r code-line-numbers="|2|3-6|"}
library(selenider)
# open the session
session <- selenider_session(
  "chromote",
  timeout = 10
)
```
::: {.fragment .fade-in}
*   This launches a browser controlled by Selenider.
*   You can customize options like the browser (Chrome by default) and timeout.
:::



## Great after we now know how to setup we can move to the important things{.center auto-animate="true"}

## Great after we now know how to setup we can move to the important things{auto-animate="true" }

::: {.fragment .fade-in}
::: {.fragment .highlight-current-green}
::: {.fragment .semi-fade-out}

:::
:::
:::

::: {.fragment .fade-in}
::: {.fragment .highlight-current-green}
::: {.fragment .semi-fade-out}
:::
:::
:::

::: {.fragment .fade-in}
::: {.fragment .highlight-current-green}
::: {.fragment .semi-fade-out}
:::
:::
:::

# Navigating and Selecting Elements

## Opening a URL

*   Use `open_url()` to navigate to the target website:

```r
open_url("https://www.example.com")
```

## Selecting Elements

*   `s()` selects a single element using a CSS selector:

```r
header <- s("#main-header")
```

*   `ss()` selects multiple elements:

```r
links <- ss("a")
```

*   You can also use XPath selectors.

## Finding Child Elements

*   Use `find_element()` and `find_elements()` to target elements within a previously selected element.
*   Chain these with the pipe operator (`|>`) for clear element paths:

```r
title <- s("#content") |> find_element("h1")
```

# Extracting Data

## Element Properties

*   `elem_text()` extracts the text content of an element:

```r
product_name <- s(".product-title") |> elem_text()
```

*   `elem_attr()` extracts an attribute value:

```r
link_url <- s("a") |> elem_attr("href")
```

## Interacting with Elements

*   `elem_click()` simulates a mouse click:

```r
s("#load-more-button") |> elem_click()
```

*   This allows you to trigger dynamic content loading.

# Waiting and Expectations

## Waiting for Elements

*   Dynamic content may take time to load.
*   `elem_wait_until()` waits for a condition to be met:

```r
elem_wait_until(is_visible(s("#dynamic-content")))
```

## Testing Conditions

*   `elem_expect()` ensures elements meet certain criteria:

```r
s("#results") |> elem_expect(has_at_least(5))
```

# Live Tutorial

*   **Let's apply these concepts to a live web scraping scenario!**
*   **Open the prepared practice materials.**
*   **We'll walk through scraping a website with dynamic content.**

 
