---
title: "Dynamic Web Scraping with Selenider"
author: 
    - "Paul Elvis Otto"
    - "Polina Ianina"
    - "Aditi Joshi"
format:
  revealjs: 
    theme: [dark, styles.scss]
    title-slide-attributes: 
      data-background-color: "#4e4e4e"
      data-background-image: none

eval: false
---

# Introduction 

## Static and Dynamic?{.center}


:::{.center}
:::{.fragment .fade-in-then-semi-out}

What is the difference?

:::
:::

---





:::: {.columns}
::: {.column width="50%" .incremental}
Classical Webscraping

- HTML is static and fully loaded on the server side
- Classical use cases: Blogs, Newspapers
- No interaction with the elements of the site to load content
:::
::: {.column width="50%" .incremental}
Dynamic Webscraping

- Content is rendered via JavaScript on the client side
- Infinite scrolling sites, Single-page applications
- Interactions like click, scroll, and so on
:::
::::



# So whats the problem here? 

## So whats the problem here?{.center}

::: {.fragment .fade-in}
::: {.fragment .highlight-red}
::: {.fragment .semi-fade-out}
When content is not rendered, we can't just access it
:::
:::
:::

::: {.fragment .fade-in}
::: {.fragment .highlight-green}
Therefore we need to simulate the interaction, like a person with a browser
:::
:::

# How can we do that?

# With Selenider



## Why Selenider?

*   **Selenider** is an R package for browser automation and web scraping.
*   It provides a simple and powerful way to interact with websites like a human user.
*   We can select elements, extract data, and even simulate user actions like clicking and scrolling.

# Setting Up Selenider

## Installation{auto-animate="true"}
Selenider often throws problems when installing. To tackle these, we will do the following

## Installation{auto-animate="true"}

Selenider often throws problems when installing. To tackle these, we will do the following


```{.r code-line-numbers="1|2|3|"}
install.packages("renv")
library(renv)
renv::init()
renv::activate()
```

## Installation{auto-animate="true"}

Selenider often throws problems when installing. To tackle these, we will do the following

```{.r code-line-numbers="5|6|7|"}
install.packages("renv")
library(renv)
renv::init()
renv::activate()
# Selenider also needs a webdirver to run
install.packages("selenider")
install.packages("chromote")

```

## Starting a Session {auto-animate="true"}


*   Load the library and start a session:

```{.r}
library(selenider)
```

## Starting a Session {auto-animate="true"}

*   Load the library and start a session:

```{.r code-line-numbers="|2|3-6|"}
library(selenider)
# open the session
session <- selenider_session(
  "chromote",
  timeout = 10
)
```
::: {.fragment .fade-in}
*   This launches a browser controlled by Selenider.
*   You can customize options like the browser (Chrome by default) and timeout.
:::



# Let's go top down!


## Let's go top down!{.center}

::: {.fragment .highlight-green}
- We want to collect the blogposts of the FDP from their website
:::

::: {.fragment .highlight-red}
- The problem, the posts are dynamicaly loaded
:::

::: {.fragment .fade-in}
- A perfect usecase for **selenider**! 
:::


## The Code{auto-animate="true"}

::: {.fragment .fade-in}
To start, we will take care of something that you will encounter a lot.
:::

::: {.fragment .fade-in-then-highlight-red }
cookie banner
:::

::: {.fragment}
```{.r}
# Make the Session
session <- selenider_session(
  "chromote",
  timeout = 10
)
```
:::


## The Code{auto-animate="true"}

```{.r}
# Make the Session
session <- selenider_session(
  "chromote",
  timeout = 10
)

# Open the FDP blog article overview page
open_url("https://www.fdp.de/uebersicht/artikel")

# Handle the cookie banner if it appears (check if it is present)
cookiebanner <- s('#consent-overlay-submit-all')

# Check if the cookie banner is present before clicking
if (elem_wait_until(cookiebanner, is_present, timeout = 5)) {
  elem_click(cookiebanner)
  reload()
}
```
## The Code

::: {.fragment}
So what did we do?
:::

::: {.fragment}
::: {.incremental}
::: {.fade-in-then-semi-out}
- We used `open_url` to open the website.
- Then we used the `s()` selector to get an element.
- Next, we took that cookie element `cookiebanner`.
- Checked whether it is there, and then clicked it with `elem_click`.
:::
:::
:::

## The Code{.center}

Great! Now that we have handled that, we can move to the fun part.

## Selecting posts




```{.r code-line-numbers="2|5|8|22-39|56-71"}
# Loop for 3 pages (adjust as needed)
for (i in 1:3) {
  
  # Wait for articles to be present
  elem_wait_until(s("a.mode-search"), is_present, timeout = 10)
  
  # Extract all articles
  articles <- ss("a.mode-search")
  
  # Check if articles were found
  if (length(articles) == 0) {
    break  # Stop if no articles are found
  }
  
  # Initialize a list to store article information
  article_info <- list()
  
  # Loop through each article and extract information
  for (j in seq_along(articles)) {
    article <- articles[[j]]
    
    info <- tryCatch({
      # Extract the URL
      url <- elem_attr(article, "href")
      
      # Extract the title
      title_elem <- find_element(article, "h3")
      title <- if (elem_wait_until(title_elem, is_present, timeout = 5)) elem_text(title_elem) else NA
      
      # Extract the date
      date_elem <- find_element(article, "time")
      date <- if (elem_wait_until(date_elem, is_present, timeout = 5)) elem_attr(date_elem, "datetime") else NA
      
      # Extract the summary
      summary_elem <- find_element(article, "p")
      summary <- if (elem_wait_until(summary_elem, is_present, timeout = 5)) elem_text(summary_elem) else NA
      
      # Return as a named list
      list(link = url, title = title, date = date, summary = summary)
      
    }, error = function(e) {
      # Return NA values in case of error
      list(link = NA, title = NA, date = NA, summary = NA)
    })
    
    # Append the info to the list
    article_info[[j]] <- info
  }
  
  # Combine the article info into a data frame
  page_info <- bind_rows(article_info)
  
  # Add this page's articles to the overall data frame
  all_blog_info <- bind_rows(all_blog_info, page_info)
  
  # Attempt to select the "Next" button
  next_button <- s("a[rel='next']")
  
  # Check if the "Next" button is present before clicking
  if (!elem_wait_until(next_button, is_present, timeout = 5)) {
    break  # Stop if there is no next button
  }
  
  # Scroll to the "Next" button before clicking
  elem_scroll_to(next_button)
  
  # Click the "Next" button
  elem_click(next_button)
  
  # Wait for the next page to load
  elem_wait_until(s("a.mode-search"), is_present, timeout = 10)
}

# Show the data frame with all articles, titles, dates, and summaries
print(all_blog_info)```
```














